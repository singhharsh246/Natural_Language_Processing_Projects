{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXhy9ovOWnwf",
        "outputId": "75ddc68d-c1f3-4e47-fbe9-ab53f6b5888c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-06 18:38:06--  https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv\n",
            "Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.21.23.210, 172.67.213.166, 2606:4700:3030::ac43:d5a6, ...\n",
            "Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.21.23.210|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5085081 (4.8M) [text/csv]\n",
            "Saving to: ‘bbc_text_cls.csv’\n",
            "\n",
            "bbc_text_cls.csv    100%[===================>]   4.85M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-05-06 18:38:06 (59.5 MB/s) - ‘bbc_text_cls.csv’ saved [5085081/5085081]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#### loading the dataset #####\n",
        "\n",
        "!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, Input\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xHs5At2gW0kV"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### reading the csv file #####\n",
        "\n",
        "bbc_text = pd.read_csv('bbc_text_cls.csv')"
      ],
      "metadata": {
        "id": "BG-3vu4qW4nH"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_text.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jFsCTdxlXCYv",
        "outputId": "b273d248-0c13-450d-d9a1-aadbd0569078"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text    labels\n",
              "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
              "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
              "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
              "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
              "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce0a9d79-4017-40c1-8e94-88aa543c625f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce0a9d79-4017-40c1-8e94-88aa543c625f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce0a9d79-4017-40c1-8e94-88aa543c625f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce0a9d79-4017-40c1-8e94-88aa543c625f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### different labels available ######\n",
        "\n",
        "set(bbc_text['labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZGBtAIgXEU2",
        "outputId": "93331b6d-fff3-4558-cde8-417ebab3c8b2"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'business', 'entertainment', 'politics', 'sport', 'tech'}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### selecting only sport news since we want topic specific suggestions #####\n",
        "\n",
        "sport_df = bbc_text[bbc_text['labels'] == 'sport']"
      ],
      "metadata": {
        "id": "oBefgmbFXInD"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sport_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XCjLBhIKXVUv",
        "outputId": "f7fd7c65-dfed-4511-87e9-d88b5951a40e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text labels\n",
              "1313  Claxton hunting first major medal\\n\\nBritish h...  sport\n",
              "1314  O'Sullivan could run in Worlds\\n\\nSonia O'Sull...  sport\n",
              "1315  Greene sets sights on world title\\n\\nMaurice G...  sport\n",
              "1316  IAAF launches fight against drugs\\n\\nThe IAAF ...  sport\n",
              "1317  Dibaba breaks 5,000m world record\\n\\nEthiopia'...  sport"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d365d046-2f3a-464f-b3fb-7c6694ab6470\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1313</th>\n",
              "      <td>Claxton hunting first major medal\\n\\nBritish h...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1314</th>\n",
              "      <td>O'Sullivan could run in Worlds\\n\\nSonia O'Sull...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315</th>\n",
              "      <td>Greene sets sights on world title\\n\\nMaurice G...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1316</th>\n",
              "      <td>IAAF launches fight against drugs\\n\\nThe IAAF ...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>Dibaba breaks 5,000m world record\\n\\nEthiopia'...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d365d046-2f3a-464f-b3fb-7c6694ab6470')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d365d046-2f3a-464f-b3fb-7c6694ab6470 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d365d046-2f3a-464f-b3fb-7c6694ab6470');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = sport_df['text']"
      ],
      "metadata": {
        "id": "709fYUGcXWV-"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = corpus[0:100] ### selecting first 100 sentences for faster training ######"
      ],
      "metadata": {
        "id": "VzWZE14FeWKd"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the vocabulary\n",
        "vocab = set()\n",
        "for sentence in corpus:\n",
        "    for word in sentence.split():\n",
        "        vocab.add(word)\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "25jNKs1NXZbA"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the corpus\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)"
      ],
      "metadata": {
        "id": "p9mQu3lHXaEs"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wIcOrpvUdJP5"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "\n",
        "embedding_dim = 100\n",
        "window_size = 2\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "SpXRyhZwXgyJ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate word pairs as input-output pairs for skip-gram\n",
        "word_pairs = []\n",
        "for sequence in sequences:\n",
        "    for i in range(len(sequence)):\n",
        "        for j in range(max(0, i - window_size), min(i + window_size + 1, len(sequence))):\n",
        "            if i != j:\n",
        "                word_pairs.append((sequence[i], sequence[j]))"
      ],
      "metadata": {
        "id": "AO_wPZYUXnHR"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input and output data\n",
        "\n",
        "input_data, output_data = zip(*word_pairs)\n",
        "input_data = np.array(input_data)\n",
        "output_data = np.array(output_data)\n"
      ],
      "metadata": {
        "id": "FXCoMZsNYWLq"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "input_layer = Input(shape=(1,))\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim)(input_layer)\n",
        "output_layer = Dense(vocab_size, activation='softmax')(embedding_layer)\n",
        "\n",
        "### using softmax as it is the standard algorithm to convert a input of K numbers into a probability distribution of K possible outcomes, which is what we need for this exercise ####\n"
      ],
      "metadata": {
        "id": "vuNpKoLQYrYa"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the model ##\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "HiCN6CFpYtko"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(input_data, output_data, epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu4M8mvScMIS",
        "outputId": "ee9ef774-65d1-4473-f79a-d1166c74f7d4"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 128304 samples\n",
            "Epoch 1/100\n",
            "128304/128304 [==============================] - 68s 529us/sample - loss: 6.9126\n",
            "Epoch 2/100\n",
            "128304/128304 [==============================] - 67s 520us/sample - loss: 6.3699\n",
            "Epoch 3/100\n",
            "128304/128304 [==============================] - 66s 516us/sample - loss: 6.0972\n",
            "Epoch 4/100\n",
            "128304/128304 [==============================] - 66s 515us/sample - loss: 5.8462\n",
            "Epoch 5/100\n",
            "128304/128304 [==============================] - 67s 520us/sample - loss: 5.6255\n",
            "Epoch 6/100\n",
            "128304/128304 [==============================] - 68s 528us/sample - loss: 5.4356\n",
            "Epoch 7/100\n",
            "128304/128304 [==============================] - 67s 521us/sample - loss: 5.2719\n",
            "Epoch 8/100\n",
            "128304/128304 [==============================] - 68s 526us/sample - loss: 5.1323\n",
            "Epoch 9/100\n",
            "128304/128304 [==============================] - 68s 526us/sample - loss: 5.0152\n",
            "Epoch 10/100\n",
            "128304/128304 [==============================] - 67s 523us/sample - loss: 4.9178\n",
            "Epoch 11/100\n",
            "128304/128304 [==============================] - 69s 536us/sample - loss: 4.8378\n",
            "Epoch 12/100\n",
            "128304/128304 [==============================] - 70s 544us/sample - loss: 4.7739\n",
            "Epoch 13/100\n",
            "128304/128304 [==============================] - 71s 555us/sample - loss: 4.7215\n",
            "Epoch 14/100\n",
            "128304/128304 [==============================] - 72s 561us/sample - loss: 4.6804\n",
            "Epoch 15/100\n",
            "128304/128304 [==============================] - 70s 547us/sample - loss: 4.6474\n",
            "Epoch 16/100\n",
            "128304/128304 [==============================] - 70s 543us/sample - loss: 4.6211\n",
            "Epoch 17/100\n",
            "128304/128304 [==============================] - 70s 547us/sample - loss: 4.6003\n",
            "Epoch 18/100\n",
            "128304/128304 [==============================] - 68s 533us/sample - loss: 4.5839\n",
            "Epoch 19/100\n",
            "128304/128304 [==============================] - 68s 532us/sample - loss: 4.5707\n",
            "Epoch 20/100\n",
            "128304/128304 [==============================] - 70s 549us/sample - loss: 4.5604\n",
            "Epoch 21/100\n",
            "128304/128304 [==============================] - 68s 531us/sample - loss: 4.5507\n",
            "Epoch 22/100\n",
            "128304/128304 [==============================] - 66s 518us/sample - loss: 4.5442\n",
            "Epoch 23/100\n",
            "128304/128304 [==============================] - 66s 518us/sample - loss: 4.5378\n",
            "Epoch 24/100\n",
            "128304/128304 [==============================] - 67s 522us/sample - loss: 4.5334\n",
            "Epoch 25/100\n",
            "128304/128304 [==============================] - 67s 518us/sample - loss: 4.5294\n",
            "Epoch 26/100\n",
            "128304/128304 [==============================] - 66s 516us/sample - loss: 4.5255\n",
            "Epoch 27/100\n",
            "128304/128304 [==============================] - 66s 513us/sample - loss: 4.5228\n",
            "Epoch 28/100\n",
            "128304/128304 [==============================] - 66s 511us/sample - loss: 4.5198\n",
            "Epoch 29/100\n",
            "128304/128304 [==============================] - 66s 513us/sample - loss: 4.5180\n",
            "Epoch 30/100\n",
            "128304/128304 [==============================] - 66s 513us/sample - loss: 4.5156\n",
            "Epoch 31/100\n",
            "128304/128304 [==============================] - 66s 514us/sample - loss: 4.5137\n",
            "Epoch 32/100\n",
            "128304/128304 [==============================] - 66s 512us/sample - loss: 4.5133\n",
            "Epoch 33/100\n",
            "128304/128304 [==============================] - 66s 512us/sample - loss: 4.5121\n",
            "Epoch 34/100\n",
            "128304/128304 [==============================] - 66s 512us/sample - loss: 4.5108\n",
            "Epoch 35/100\n",
            "128304/128304 [==============================] - 66s 514us/sample - loss: 4.5095\n",
            "Epoch 36/100\n",
            "128304/128304 [==============================] - 65s 509us/sample - loss: 4.5087\n",
            "Epoch 37/100\n",
            "128304/128304 [==============================] - 66s 513us/sample - loss: 4.5076\n",
            "Epoch 38/100\n",
            "128304/128304 [==============================] - 66s 515us/sample - loss: 4.5080\n",
            "Epoch 39/100\n",
            "128304/128304 [==============================] - 66s 514us/sample - loss: 4.5071\n",
            "Epoch 40/100\n",
            "128304/128304 [==============================] - 66s 511us/sample - loss: 4.5065\n",
            "Epoch 41/100\n",
            "128304/128304 [==============================] - 68s 526us/sample - loss: 4.5058\n",
            "Epoch 42/100\n",
            "128304/128304 [==============================] - 74s 579us/sample - loss: 4.5061\n",
            "Epoch 43/100\n",
            "128304/128304 [==============================] - 77s 601us/sample - loss: 4.5058\n",
            "Epoch 44/100\n",
            "128304/128304 [==============================] - 72s 560us/sample - loss: 4.5051\n",
            "Epoch 45/100\n",
            "128304/128304 [==============================] - 68s 530us/sample - loss: 4.5049\n",
            "Epoch 46/100\n",
            "128304/128304 [==============================] - 67s 521us/sample - loss: 4.5053\n",
            "Epoch 47/100\n",
            "128304/128304 [==============================] - 67s 522us/sample - loss: 4.5048\n",
            "Epoch 48/100\n",
            "128304/128304 [==============================] - 67s 523us/sample - loss: 4.5044\n",
            "Epoch 49/100\n",
            "128304/128304 [==============================] - 67s 522us/sample - loss: 4.5044\n",
            "Epoch 50/100\n",
            "128304/128304 [==============================] - 68s 533us/sample - loss: 4.5049\n",
            "Epoch 51/100\n",
            "128304/128304 [==============================] - 67s 525us/sample - loss: 4.5044\n",
            "Epoch 52/100\n",
            "128304/128304 [==============================] - 67s 521us/sample - loss: 4.5043\n",
            "Epoch 53/100\n",
            "128304/128304 [==============================] - 67s 521us/sample - loss: 4.5043\n",
            "Epoch 54/100\n",
            "128304/128304 [==============================] - 68s 529us/sample - loss: 4.5042\n",
            "Epoch 55/100\n",
            "128304/128304 [==============================] - 68s 531us/sample - loss: 4.5044\n",
            "Epoch 56/100\n",
            "128304/128304 [==============================] - 68s 527us/sample - loss: 4.5047\n",
            "Epoch 57/100\n",
            "128304/128304 [==============================] - 68s 527us/sample - loss: 4.5042\n",
            "Epoch 58/100\n",
            "128304/128304 [==============================] - 68s 527us/sample - loss: 4.5039\n",
            "Epoch 59/100\n",
            "128304/128304 [==============================] - 69s 536us/sample - loss: 4.5041\n",
            "Epoch 60/100\n",
            "128304/128304 [==============================] - 69s 537us/sample - loss: 4.5048\n",
            "Epoch 61/100\n",
            "128304/128304 [==============================] - 66s 512us/sample - loss: 4.5045\n",
            "Epoch 62/100\n",
            "128304/128304 [==============================] - 67s 520us/sample - loss: 4.5052\n",
            "Epoch 63/100\n",
            "128304/128304 [==============================] - 67s 520us/sample - loss: 4.5046\n",
            "Epoch 64/100\n",
            "128304/128304 [==============================] - 66s 516us/sample - loss: 4.5058\n",
            "Epoch 65/100\n",
            "128304/128304 [==============================] - 66s 511us/sample - loss: 4.5050\n",
            "Epoch 66/100\n",
            "128304/128304 [==============================] - 66s 511us/sample - loss: 4.5057\n",
            "Epoch 67/100\n",
            "128304/128304 [==============================] - 66s 512us/sample - loss: 4.5052\n",
            "Epoch 68/100\n",
            "128304/128304 [==============================] - 65s 510us/sample - loss: 4.5058\n",
            "Epoch 69/100\n",
            "128304/128304 [==============================] - 66s 511us/sample - loss: 4.5061\n",
            "Epoch 70/100\n",
            "128304/128304 [==============================] - 66s 515us/sample - loss: 4.5059\n",
            "Epoch 71/100\n",
            "128304/128304 [==============================] - 66s 514us/sample - loss: 4.5065\n",
            "Epoch 72/100\n",
            "128304/128304 [==============================] - 65s 510us/sample - loss: 4.5064\n",
            "Epoch 73/100\n",
            "128304/128304 [==============================] - 66s 512us/sample - loss: 4.5068\n",
            "Epoch 74/100\n",
            "128304/128304 [==============================] - 66s 515us/sample - loss: 4.5064\n",
            "Epoch 75/100\n",
            "128304/128304 [==============================] - 65s 509us/sample - loss: 4.5075\n",
            "Epoch 76/100\n",
            "128304/128304 [==============================] - 65s 507us/sample - loss: 4.5068\n",
            "Epoch 77/100\n",
            "128304/128304 [==============================] - 65s 507us/sample - loss: 4.5083\n",
            "Epoch 78/100\n",
            "128304/128304 [==============================] - 65s 504us/sample - loss: 4.5074\n",
            "Epoch 79/100\n",
            "128304/128304 [==============================] - 65s 505us/sample - loss: 4.5086\n",
            "Epoch 80/100\n",
            "128304/128304 [==============================] - 65s 507us/sample - loss: 4.5079\n",
            "Epoch 81/100\n",
            "128304/128304 [==============================] - 65s 509us/sample - loss: 4.5081\n",
            "Epoch 82/100\n",
            "128304/128304 [==============================] - 65s 506us/sample - loss: 4.5086\n",
            "Epoch 83/100\n",
            "128304/128304 [==============================] - 64s 498us/sample - loss: 4.5089\n",
            "Epoch 84/100\n",
            "128304/128304 [==============================] - 64s 500us/sample - loss: 4.5082\n",
            "Epoch 85/100\n",
            "128304/128304 [==============================] - 65s 504us/sample - loss: 4.5095\n",
            "Epoch 86/100\n",
            "128304/128304 [==============================] - 65s 507us/sample - loss: 4.5095\n",
            "Epoch 87/100\n",
            "128304/128304 [==============================] - 65s 506us/sample - loss: 4.5087\n",
            "Epoch 88/100\n",
            "128304/128304 [==============================] - 66s 514us/sample - loss: 4.5099\n",
            "Epoch 89/100\n",
            "128304/128304 [==============================] - 74s 577us/sample - loss: 4.5095\n",
            "Epoch 90/100\n",
            "128304/128304 [==============================] - 66s 513us/sample - loss: 4.5099\n",
            "Epoch 91/100\n",
            "128304/128304 [==============================] - 65s 510us/sample - loss: 4.5108\n",
            "Epoch 92/100\n",
            "128304/128304 [==============================] - 65s 505us/sample - loss: 4.5107\n",
            "Epoch 93/100\n",
            "128304/128304 [==============================] - 65s 508us/sample - loss: 4.5109\n",
            "Epoch 94/100\n",
            "128304/128304 [==============================] - 64s 496us/sample - loss: 4.5115\n",
            "Epoch 95/100\n",
            "128304/128304 [==============================] - 64s 495us/sample - loss: 4.5110\n",
            "Epoch 96/100\n",
            "128304/128304 [==============================] - 63s 491us/sample - loss: 4.5115\n",
            "Epoch 97/100\n",
            "128304/128304 [==============================] - 64s 496us/sample - loss: 4.5115\n",
            "Epoch 98/100\n",
            "128304/128304 [==============================] - 64s 496us/sample - loss: 4.5115\n",
            "Epoch 99/100\n",
            "128304/128304 [==============================] - 64s 495us/sample - loss: 4.5127\n",
            "Epoch 100/100\n",
            "128304/128304 [==============================] - 63s 493us/sample - loss: 4.5129\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7c4e865f90>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the learned word embeddings\n",
        "\n",
        "weights = model.layers[1].get_weights()[0]"
      ],
      "metadata": {
        "id": "-WsEg_iYcnI3"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print the word embeddings\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < vocab_size:\n",
        "        embedding_matrix[i] = weights[i]\n",
        "\n",
        "print(\"Word Embeddings:\")\n",
        "for word, i in word_index.items():\n",
        "    if i < vocab_size:\n",
        "        print(word, embedding_matrix[i])"
      ],
      "metadata": {
        "id": "9JYdqfr5dg3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to find the nearest words to a given word\n",
        "def find_nearest_words(word, embeddings, vocab, n=5):\n",
        "    # Get the index of the word in the vocabulary\n",
        "    word_index = vocab.index(word)\n",
        "    # Get the word embedding\n",
        "    word_embedding = embeddings[word_index]\n",
        "    # Compute the cosine similarity between the word embedding and all other word embeddings\n",
        "    cosine_similarities = np.dot(embeddings, word_embedding) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(word_embedding))\n",
        "    # Get the indices of the most similar words\n",
        "    nearest_indices = cosine_similarities.argsort()[-n-1:-1][::-1]\n",
        "    # Get the most similar words\n",
        "    nearest_words = [vocab[i] for i in nearest_indices]\n",
        "    return nearest_words\n",
        "\n",
        "# Find the nearest words to the word 'sentence'\n",
        "nearest_words = find_nearest_words('breaks', embedding_matrix, list(vocab))\n",
        "print(\"Nearest words to 'sentence':\", nearest_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMQBkMKS1GyM",
        "outputId": "7fd158a4-3cb3-474e-bd08-264643099497"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest words to 'sentence': ['19-year-old.', 'passing', 'predict', '3:49.78.', 'who']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-94-f6d7bceb36ea>:8: RuntimeWarning: invalid value encountered in true_divide\n",
            "  cosine_similarities = np.dot(embeddings, word_embedding) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(word_embedding))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "Jf5f9Cdv1HCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### using pre-trained embeddings ######"
      ],
      "metadata": {
        "id": "NioFWNPq1HLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained word embeddings\n",
        "embedding_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gqhEyqP4km1",
        "outputId": "b5852b78-740b-43e9-a391-0f5b1bd6ea5c"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[================================================--] 96.7% 123.8/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"breaks\"\n",
        "nearest_words = embedding_model.most_similar(word)\n",
        "\n",
        "# Print the nearest words\n",
        "print(\"Nearest words to '{}':\".format(word))\n",
        "for word, similarity in nearest_words:\n",
        "    print(word, similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mvSbYKV4ksC",
        "outputId": "11cd36d1-ce1c-4fd7-bbfa-f8cd82f17a24"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest words to 'breaks':\n",
            "break 0.714804470539093\n",
            "gets 0.6286557912826538\n",
            "breaking 0.624260425567627\n",
            "goes 0.6123769283294678\n",
            "cut 0.6074079871177673\n",
            "cuts 0.6058018803596497\n",
            "takes 0.6039687991142273\n",
            "puts 0.5913140773773193\n",
            "money 0.5860028862953186\n",
            "gives 0.5848631858825684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tqP8QL-v4kyU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}